{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finnhub\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Logging for progress updates\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US stock symbols have been saved to 'us_stock_symbols.csv'\n"
     ]
    }
   ],
   "source": [
    "# Initialize Finnhub client\n",
    "finnhub_client = finnhub.Client(api_key=\"removed")\n",
    "\n",
    "# Get US stock symbols and save to a CSV file\n",
    "symbols_data = finnhub_client.stock_symbols('US')\n",
    "pd.DataFrame(symbols_data).to_csv('us_stock_symbols.csv', index=False)\n",
    "print(\"US stock symbols have been saved to 'us_stock_symbols.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load symbols from the CSV and convert to list\n",
    "us_stock_df = pd.read_csv('us_stock_symbols.csv')\n",
    "symbols = us_stock_df['symbol'].dropna().astype(str).tolist()\n",
    "\n",
    "# Define date range and batch size\n",
    "start_date, end_date = \"2024-01-01\", \"2024-01-31\"\n",
    "batch_size = 50\n",
    "\n",
    "# Create an empty DataFrame for all collected stock data\n",
    "all_stock_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result saved to 'daily_log_returns.csv'\n"
     ]
    }
   ],
   "source": [
    "# Process symbols in batches\n",
    "for i in range(0, len(symbols), batch_size):\n",
    "    batch_symbols = symbols[i:i + batch_size]\n",
    "    logging.info(f\"Processing batch {i // batch_size + 1} of {len(batch_symbols)} symbols.\")\n",
    "    \n",
    "    try:\n",
    "        # Download data for the current batch\n",
    "        batch_data = yf.download(batch_symbols, start=start_date, end=end_date, group_by='ticker', threads=True)\n",
    "        \n",
    "        # Process each stock symbol's data\n",
    "        for symbol in batch_symbols:\n",
    "            if symbol in batch_data:\n",
    "                stock_df = batch_data[symbol].reset_index()\n",
    "                stock_df['Symbol'] = symbol\n",
    "                stock_df['LogReturn'] = np.log(stock_df['Close'] / stock_df['Close'].shift(1))\n",
    "                \n",
    "                # Append to main DataFrame\n",
    "                all_stock_data = pd.concat([all_stock_data, stock_df[['Symbol', 'Date', 'Close', 'LogReturn']]])\n",
    "\n",
    "                # Calculate and log progress\n",
    "                progress = (len(all_stock_data['Symbol'].unique()) / len(symbols)) * 100\n",
    "                logging.info(f\"Progress: {progress:.2f}% complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process batch: {e}\")\n",
    "    \n",
    "    # Pause to avoid hitting rate limits\n",
    "    time.sleep(5)\n",
    "\n",
    "# Save final data to CSV\n",
    "all_stock_data.to_csv('daily_log_returns.csv', index=False)\n",
    "logging.info(\"Data collection complete. Results saved to 'daily_log_returns.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Cleaned data saved to 'cleaned_log_returns.csv' with rows missing both 'Close' and 'LogReturn' removed.\n"
     ]
    }
   ],
   "source": [
    "# Load the daily log returns data\n",
    "daily_log_returns = pd.read_csv('daily_log_returns.csv')\n",
    "\n",
    "# Drop rows where both 'Close' and 'LogReturn' are NaN\n",
    "cleaned_data = daily_log_returns.dropna(subset=['Close', 'LogReturn'], how='all').reset_index(drop=True)\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "cleaned_data.to_csv('cleaned_log_returns.csv', index=False)\n",
    "logging.info(\"Cleaned data saved to 'cleaned_log_returns.csv' with rows missing both 'Close' and 'LogReturn' removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered log returns saved to 'filtered_log_returns.csv' with only valid LogReturn values.\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned log returns data\n",
    "cleaned_data = pd.read_csv('cleaned_log_returns.csv')\n",
    "\n",
    "# Filter rows where 'LogReturn' is not NaN (i.e., valid log return values are present)\n",
    "filtered_log_returns = cleaned_data.dropna(subset=['LogReturn'])\n",
    "\n",
    "# Save this filtered dataset to a new CSV file\n",
    "filtered_log_returns.to_csv('filtered_log_returns.csv', index=False)\n",
    "print(\"Filtered log returns saved to 'filtered_log_returns.csv' with only valid LogReturn values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Cleaned aggregated log returns summary saved to 'cleaned_aggregated_log_returns.csv' without rows of all-zero statistics.\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned log returns data\n",
    "cleaned_data = pd.read_csv('cleaned_log_returns.csv')\n",
    "\n",
    "# Group by stock symbol and calculate summary statistics for log returns\n",
    "aggregated_stats = cleaned_data.groupby('Symbol')['LogReturn'].agg(\n",
    "    MeanLogReturn='mean',\n",
    "    StdLogReturn='std',\n",
    "    MinLogReturn='min',\n",
    "    MaxLogReturn='max'\n",
    ").reset_index()\n",
    "\n",
    "# Filter out rows where all statistics are zero\n",
    "aggregated_stats = aggregated_stats[\n",
    "    ~(aggregated_stats[['MeanLogReturn', 'StdLogReturn', 'MinLogReturn', 'MaxLogReturn']] == 0).all(axis=1)\n",
    "]\n",
    "\n",
    "# Save the summary statistics to a new CSV file\n",
    "aggregated_stats.to_csv('cleaned_aggregated_log_returns.csv', index=False)\n",
    "logging.info(\"Cleaned aggregated log returns summary saved to 'cleaned_aggregated_log_returns.csv' without rows of all-zero statistics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data with nonzero log returns saved to 'filtered_nonzero_log_returns.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned log returns data\n",
    "data = pd.read_csv('cleaned_log_returns.csv')\n",
    "\n",
    "# Filter out rows where LogReturn is NaN or 0.0\n",
    "filtered_data = data[(data['LogReturn'].notna()) & (data['LogReturn'] != 0.0)]\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "filtered_data.to_csv('filtered_nonzero_log_returns.csv', index=False)\n",
    "\n",
    "print(\"Filtered data with nonzero log returns saved to 'filtered_nonzero_log_returns.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled data with zero log returns saved to 'sampled_filtered_log_returns.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the filtered log returns data (includes zero log returns)\n",
    "filtered_log_returns = pd.read_csv('filtered_log_returns.csv')\n",
    "\n",
    "# Get a list of unique tickers and shuffle them\n",
    "unique_tickers_incl_zero = filtered_log_returns['Symbol'].unique()\n",
    "np.random.seed(42)  # for reproducibility\n",
    "np.random.shuffle(unique_tickers_incl_zero)\n",
    "\n",
    "# Randomly select tickers until the total rows reach approximately 5,000\n",
    "selected_tickers_incl_zero = []\n",
    "total_rows_incl_zero = 0\n",
    "\n",
    "# Loop through the shuffled tickers until around 5,000 rows are selected\n",
    "for ticker in unique_tickers_incl_zero:\n",
    "    sampled_ticker_data = filtered_log_returns[filtered_log_returns['Symbol'] == ticker]\n",
    "    selected_tickers_incl_zero.append(sampled_ticker_data)\n",
    "    total_rows_incl_zero += len(sampled_ticker_data)\n",
    "    \n",
    "    # Stop once we reach around 5,000 rows\n",
    "    if total_rows_incl_zero >= 5000:\n",
    "        break\n",
    "\n",
    "# Combine selected tickers into a DataFrame\n",
    "sampled_incl_zero = pd.concat(selected_tickers_incl_zero, ignore_index=True)\n",
    "\n",
    "# Save the sampled data to a new CSV file\n",
    "sampled_incl_zero.to_csv('sampled_filtered_log_returns.csv', index=False)\n",
    "print(\"Sampled data with zero log returns saved to 'sampled_filtered_log_returns.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled data without zero log returns saved to 'sampled_filtered_nonzero_log_returns.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the filtered nonzero log returns data (excludes zero log returns)\n",
    "filtered_nonzero_log_returns = pd.read_csv('filtered_nonzero_log_returns.csv')\n",
    "\n",
    "# Get a list of unique tickers and shuffle them\n",
    "unique_tickers_nonzero = filtered_nonzero_log_returns['Symbol'].unique()\n",
    "np.random.seed(42)  # for reproducibility\n",
    "np.random.shuffle(unique_tickers_nonzero)\n",
    "\n",
    "# Randomly select tickers until the total rows reach approximately 5,000\n",
    "selected_tickers_nonzero = []\n",
    "total_rows_nonzero = 0\n",
    "\n",
    "# Loop through the shuffled tickers until around 5,000 rows are selected\n",
    "for ticker in unique_tickers_nonzero:\n",
    "    sampled_ticker_data = filtered_nonzero_log_returns[filtered_nonzero_log_returns['Symbol'] == ticker]\n",
    "    selected_tickers_nonzero.append(sampled_ticker_data)\n",
    "    total_rows_nonzero += len(sampled_ticker_data)\n",
    "    \n",
    "    # Stop once we reach around 5,000 rows\n",
    "    if total_rows_nonzero >= 5000:\n",
    "        break\n",
    "\n",
    "# Combine selected tickers into a DataFrame\n",
    "sampled_excl_zero = pd.concat(selected_tickers_nonzero, ignore_index=True)\n",
    "\n",
    "# Save the sampled data to a new CSV file\n",
    "sampled_excl_zero.to_csv('sampled_filtered_nonzero_log_returns.csv', index=False)\n",
    "print(\"Sampled data without zero log returns saved to 'sampled_filtered_nonzero_log_returns.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65b627fa2e534343e3a4ce75b139528ebeefe3065edf307f852d412da28151cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
